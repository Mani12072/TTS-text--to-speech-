# -*- coding: utf-8 -*-
"""4bharat / vits_rasa_13 -TTS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1967PL4Cl4W2e12AHXfXuJ26w48Qoni9I

###Installing Required Librires
"""

!pip install fastapi uvicorn pyngrok nest-asyncio
!pip install transformers
!pip install git+https://github.com/huggingface/parler-tts.git

import os

from google.colab import userdata
hf_key=userdata.get('hf3')

os.environ["HF_TOKEN"] = hf_key
os.environ["HUGGINGFACEHUB_API_TOKEN"] = os.environ["HF_TOKEN"]

# Replace YOUR_AUTHTOKEN with your real token
!ngrok authtoken 34VTmzzy3SPSjs7scuFqU9bFgVE_3bchMnebqVrvrJtmUaECT

import io
import uvicorn
import asyncio
import torch
import nest_asyncio
from pyngrok import ngrok
from fastapi import FastAPI
from fastapi.responses import Response
from pydantic import BaseModel
from transformers import pipeline
from transformers import AutoModel
from transformers import AutoTokenizer
import soundfile as sf

!pip install transformers torch

class TTS(BaseModel):
    prompt: str
    speaker_id: int
    style_id: int

nest_asyncio.apply()

# Load model & tokenizer once
model_name = "ai4bharat/vits_rasa_13"
model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

app=FastAPI()

@app.get("/")
def wel_page():
  return "Welcome to TTS API"

@app.post("/tts")
def text_to_speech(request: TTS):
    try:
        text = request.prompt
        speaker_id = request.speaker_id
        style_id = request.style_id

        # Tokenize
        inputs = tokenizer(text=text, return_tensors="pt").to(device)

        # Inference (as in your working example)
        with torch.no_grad():
            outputs = model(inputs["input_ids"], speaker_id=speaker_id, emotion_id=style_id)

        # Extract waveform
        audio_arr = outputs.waveform.squeeze().cpu().numpy()

        # Save to memory as WAV
        output_buffer = io.BytesIO()
        sf.write(output_buffer, audio_arr, model.config.sampling_rate, format="WAV")
        output_buffer.seek(0)

        # Return the audio as HTTP response
        return Response(content=output_buffer.read(), media_type="audio/wav")

    except Exception as e:
        return {"error": str(e)}

# Create ngrok tunnel
public_url = ngrok.connect(8000)
print("ðŸš€ Public URL:", public_url.public_url)

# Start FastAPI server safely in Colab
config = uvicorn.Config(app=app, host="0.0.0.0", port=8000, log_level="info")
server = uvicorn.Server(config)

# âœ… Correct async execution for Colab
loop = asyncio.get_event_loop()
loop.create_task(server.serve())

# import time, subprocess
# from IPython.display import clear_output, HTML, display

# while True:
#     gpu_info = subprocess.getoutput("nvidia-smi")
#     clear_output(wait=True)
#     display(HTML(f"<pre style='background:#111;color:#0f0;padding:10px;border-radius:10px'>{gpu_info}</pre>"))
#     time.sleep(5)